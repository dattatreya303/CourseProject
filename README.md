# Summary Extraction from Spotify Podcasts

## Description
This repository contains the code and reports used in CS410 Fa21 course project. This document describes the usage of all scripts and functions defined in them. For more information about the project tasks, please see the the [proposal](CS_410_TIS_Project_proposal.pdf) and [progress-report](CS_410_TIS_Project_Progress_Report.pdf).

## File Descriptions
| File Name | Task Summary |
| ----------- | ----------- |
| [`extract_data.py`](extract_data.py) | Extract episode transcripts and creator descriptions from the raw dataset. Process them combined CSVs using episode prefix as unique id|
| [`content_selection.py`](content_selection.py) | Reduces episode transcripts to the top 5 most important sentences according to TextRank algorithm |
| [`t5_training.py`](t5_training.py) | Script for fine tuning the T5 transformer model on the preprocessed podcast dataset |

Each of the above files are accompanied by a corresponding `ipynb` notebook file as well. CAs/TAs are requested to use those notebooks for evaluation. They can be imported into a local jupyter instance or Google Colab

## Function Descriptions:

### `extract_episode_lists_and_transcripts`
Source: [`extract_data.py`](extract_data.py)

Inputs: `data_id` (int) Each of the raw data gzip files decompress into `spotify-podcasts-2020/podcasts-transcripts/<data_id>`.

Output: tuple of two dicts - `show_episodes_dict` (Schema: `{show_prefix : [episode_prefix]}`) and `episode_transcript_dict` (Schema: `{episode_prefix: transcript_text}`)

### `generate_summ_dataset`
Source: [`extract_data.py`](extract_data.py)

Inputs: None

Outputs: Reads the dictionaries generated by `extract_episode_lists_and_transcripts` and writes a merged CSV file with the columns `(ep_prefix, ep_transcript, ep_description)`

### <another-function-X-in-content-selection.py>
Input: None
Outputs: Loads the merged CSV generated by `generate_summ_dataset` into a Pandas dataframe. Adds a new `n_sentences` (number of sentences in transcript) column.

### `text_rank_selection`
Source: [`content_selection.py`](content_selection.py)

Inputs: `transcript` (`str`), `n` (`int`)

Outputs:  Uses `summa` library to get the top 5 sentences from episode transcript text. Uses `n` to determine the percentage of sentences to feed into the `ratio` param in `summa.summarizer(ratio=)`

### `get_word_len`
Source: [`t5_training.py`](t5_training.py)

Inputs: `row` (`Series`)

Outputs:  Returns the length of the given podcast transcript.

### `cap_word_len`
Source: [`t5_training.py`](t5_training.py)

Inputs: `row` (`Series`)

Outputs:  Caps the transcript length to 7000 words if it is longer, else returns the row as it is

### `train`
Source: [`t5_training.py`](t5_training.py)

Inputs: `epoch` (`int`), `model` (`Model`), `tokenizer` (`Model Tokenizer`), `device` (`CPU/GPU`), `loader` (`CustomDataloader`), `optimizer` (`torch Optimizer`)

Outputs: Defines one step of training for the the given transformer model for one epoch based on the batch size parameters passed to the custom dataloader. Optimization of parameters happens via passed optimizer. 


### `validate`
Source: [`t5_training.py`](t5_training.py)

Inputs: `epoch` (`int`), `model` (`Model`), `tokenizer` (`Model Tokenizer`), `device` (`CPU/GPU`), `loader` (`CustomDataloader`)

Outputs: Evaluates the model for the given epoch by sampling data based on batch size parameters from the passed custom dataloader. Returns the predictions and actuals pairs.


## Usage

### Install dependencies
`pip install -r requirements.txt`

### Run data extraction
Load `extract_data.ipynb` into `jupyter` or Google Colab. Execute all cells.

### Perform content selection using TextRank
Load `content_selection.ipynb` into `jupyter` or Google Colab. Execute all cells.

### Finetune T5 model on Spotify Dataset
Load `t5_training.ipynb` into `jupyter` or Google Colab. Execute all cells.

### Tutorial Notebook for evaluation and inference on the three methods

Each of the above files are accompanied by a corresponding `ipynb` notebook file as well. CAs/TAs are requested to use those notebooks for evaluation. They can be imported into a local jupyter instance or Google Colab

## Results 

### Quantitative Results

The following table outlines our results on a subset of the validation dataset (due to computational constraints):

| Method Name | Average ROUGE F1 Score |
| ----------- | ----------- |
| TextRank | |
| T5 (Off the shelf) |  |
| T5 (Finetuned on Spotify Dataset) |  |

### Human Evaluation 

We used five English speaking volunteers to score the summaries into the defined spectrum of Bad(B) to Excellent(E), as defined by the original paper. This is so that we can effectively capture the subjectivity of how good or bad a summary is, based on how relevant it is to a human evaluator.

* 3 out of 5 people felt that the summaries generated by TextRank and Fine-Tuned were comparable, and rated it Fair(F).
* 1 evaluator felt that TextRank is definitely better, and 1 Evaluator felt that given enough data, FineTuned T5 is a much better abstraction of the podcast transcript.
* 5 out of 5 evaluators agreed that the FineTuned T5 generated better summaries than Off-the-shelf Pretrained T5. This validates our assumption about the need to perform domain adaptation.







