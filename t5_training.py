# -*- coding: utf-8 -*-
"""Copy of code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q8CtDDQXJEPEmv2_cRn7R5vBhH6yizng

**Notebook 3 : CS 410 Course Project**

*Authors: Gargi Balasubramaniam, Arijit Ghosh Chowdhury, Dattatreya Mohapatra*

# Training using T5 Transformer

This notebook shows the training methodology for the T5 transformer model on our preprocessed dataset. Note that the hyperparameters have been set to the best ones, searched during the initial coding process.

--------------------------------------------------------------------

### Importing Modules and GPU Details
"""

# Outputs the GPU settings on which the model was trained and evaluated
!nvidia-smi

!pip install transformers==2.9.0 
!pip install sentencepiece -q
!pip install wandb -q
!pip install pytorch_lightning==0.7.5

import numpy as np
import pandas as pd

import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from torch import cuda

from transformers import T5Tokenizer, T5ForConditionalGeneration

from google.colab import auth, drive
import os

# Device Setting for model training
device = 'cuda' if cuda.is_available() else 'cpu'

"""### Data Processing"""

# Google Drive Authentication 
auth.authenticate_user()
drive.mount('/content/gdrive/')

# Google Drive Folder Update

BASE_DATASET_FOLDER = "TIS/"
ROOT_DATA_PATH = "/content/gdrive/My Drive/" + BASE_DATASET_FOLDER

# Changing directory path 
os.chdir(ROOT_DATA_PATH)

"""Now, we shall load preprocessed transcript file, called `merged_prefix_transcript_description_summ.tsv`.
For running this notebook, this file can be downloaded and uploaded to colab from the shared link mentioned above.
"""

# Loading the dataset from colab's mounted drive

TSV_NAME = "merged_prefix_transcript_description_summ.tsv"

podcast_df = pd.read_csv(TSV_NAME,sep='\t',header=0)

# Removing extra index column added while loading
podcast_df = podcast_df.drop('Unnamed: 0', 1)

# Head
podcast_df.head()

"""**Dataset Statistics**"""

# Dataset Size

print("Total number of podcast episodes: {}".format(len(podcast_df)))

# Podcast Length Summary

def get_word_len(row):
  return len(row.split(' '))

podcast_df['episode_len'] = podcast_df.apply(lambda row: get_word_len(row['episode_transcript']), axis=1)

# Describe

podcast_df.describe()

"""We can see that there are `105360` data points i.e. unique episodes to train our models on. On an average, the episode transcript is `5000` words long. Taking an average of `150 words per minute`, we get an average podcast length of `33 minutes`.

**Dataset Manipulation for Compute Concerns**

Note that in order to comply with limited computational time on GPU, we restrict the number of tokens in each `episode_transcript` to 7000, as instances greater than them would lead to code crash. 

The following lines of code make use of lambda functions and `apply` on Pandas Dataframes to efficiently carry out this computation over the entire dataframe.
"""

def cap_word_len(row):
  row_len = len(row.split(' '))

  if(row_len>7000):
    row_list = row.split(' ')[:7000]
    smaller_row = ' '.join(row_list)
    return smaller_row
  else:
    return row

podcast_df['capped_episode_transcript'] = podcast_df.apply(lambda row: cap_word_len(row['episode_transcript']), axis=1)

"""Let us confirm the clipping by checking the length."""

podcast_df['capped_episode_len'] = podcast_df.apply(lambda row: get_word_len(row['capped_episode_transcript']), axis=1)

podcast_df.describe()

"""### Preparing data for train and test sets. """

from sklearn.model_selection import train_test_split

# Note that we chose to do a 60-40 split here. 
train_dataset_podcast, val_dataset_podcast = train_test_split(podcast_df, test_size=0.4)

train_dataset_podcast = train_dataset_podcast.drop(['episode_transcript','episode_len','capped_episode_len'],axis=1)

val_dataset_podcast = val_dataset_podcast.drop(['episode_transcript','episode_len','capped_episode_len'],axis=1)

# Inspecting a sample row from the train data.
print(train_dataset_podcast.iloc[1])

"""### Training with PyTorch 

We shall now train our model by defining a wrapper around the Dataset and Dataloader classes in PyTorch to define our own custom versions.
"""

class PodcastDataset(Dataset):

    def __init__(self, dataframe, tokenizer, source_len, summ_len):
        self.tokenizer = tokenizer
        self.data = dataframe
        self.source_len = source_len
        self.summ_len = summ_len
        self.text = self.data.episode_description #target summary
        self.ctext = self.data.capped_episode_transcript #original transcript

    def __len__(self):
        return len(self.text)

    def __getitem__(self, index):
        ctext = str(self.ctext.iloc[index])
        ctext = ' '.join(ctext.split())

        text = str(self.text.iloc[index])
        text = ' '.join(text.split())

        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')
        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')

        source_ids = source['input_ids'].squeeze()
        source_mask = source['attention_mask'].squeeze()
        target_ids = target['input_ids'].squeeze()
        target_mask = target['attention_mask'].squeeze()

        return {
            'source_ids': source_ids.to(dtype=torch.long), 
            'source_mask': source_mask.to(dtype=torch.long), 
            'target_ids': target_ids.to(dtype=torch.long),
            'target_ids_y': target_ids.to(dtype=torch.long)
        }


def train(epoch, tokenizer, model, device, loader, optimizer):
    model.train()
    for _,data in enumerate(loader, 0):
        #print("CHECK")
        y = data['target_ids'].to(device, dtype = torch.long)
        y_ids = y[:, :-1].contiguous()
        lm_labels = y[:, 1:].clone().detach()
        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100
        ids = data['source_ids'].to(device, dtype = torch.long)
        mask = data['source_mask'].to(device, dtype = torch.long)

        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)
        loss = outputs[0]
        
        if _%10 == 0:
            print({"Training Loss": loss.item()})


        if _%500==0:
            print(f'Epoch: {epoch}, Loss:  {loss.item()}')
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

def validate(epoch, tokenizer, model, device, loader):
    model.eval()
    predictions = []
    actuals = []
    with torch.no_grad():
        for _, data in enumerate(loader, 0):
            y = data['final'].to(device, dtype = torch.long)
            ids = data['initial'].to(device, dtype = torch.long)
            mask = data['inital_msk'].to(device, dtype = torch.long)

            generated_ids = model.generate(
                input_ids = ids,
                attention_mask = mask, 
                max_length=150, 
                num_beams=2,
                repetition_penalty=2.5, 
                length_penalty=1.0, 
                early_stopping=True
                )
            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]
            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]
            if _%100==0:
                print(f'Completed {_}')

            predictions.extend(preds)
            actuals.extend(target)
    return predictions, actuals

def main():
  
    TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)
    VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)
    TRAIN_EPOCHS = 1        # number of epochs to train (default: 10)
    VAL_EPOCHS = 1 
    LEARNING_RATE = 1e-4    # learning rate (default: 0.01)
    SEED = 42               # random seed (default: 42)
    MAX_LEN = 512
    SUMMARY_LEN = 150 

    torch.manual_seed(SEED) 
    np.random.seed(SEED) 
    torch.backends.cudnn.deterministic = True

    tokenizer = T5Tokenizer.from_pretrained("t5-base") #replace with relevant transformer model

    train_dataset= train_dataset_podcast
    val_dataset= val_dataset_podcast

    print("TRAIN Dataset: {}".format(train_dataset.shape))
    print("TEST Dataset: {}".format(val_dataset.shape))

    # Creating the Training and Validation dataset for further creation of Dataloader
    training_set = PodcastDataset(train_dataset_podcast, tokenizer, MAX_LEN, SUMMARY_LEN)
    val_set = PodcastDataset(val_dataset_podcast, tokenizer, MAX_LEN, SUMMARY_LEN)

    # Defining the parameters for creation of dataloaders
    train_params = {
        'batch_size': TRAIN_BATCH_SIZE,
        'shuffle': True,
        'num_workers': 0
        }

    val_params = {
        'batch_size': VALID_BATCH_SIZE,
        'shuffle': False,
        'num_workers': 0
        }

    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.
    training_loader = DataLoader(training_set, **train_params)
    print(training_loader)
    val_loader = DataLoader(val_set, **val_params)

    model = T5ForConditionalGeneration.from_pretrained("t5-base") #replace with relevant transformer model
    model = model.to(device)

    optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)

    print('Initiating Fine-Tuning for the model on our dataset')

    for epoch in range(TRAIN_EPOCHS):
        train(epoch, tokenizer, model, device, training_loader, optimizer)
    
    model.save_pretrained('./t5-model-3000')
    tokenizer.save_pretrained('./t5-model-3000')

if __name__ == '__main__':
    main()





